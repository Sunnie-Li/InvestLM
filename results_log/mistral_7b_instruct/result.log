nohup: ignoring input
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]
finish loading model..
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Generating!
Traceback (most recent call last):
  File "inference.py", line 266, in <module>
    fire.Fire(main)
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "inference.py", line 252, in main
    output = generator(instruction = prompt,
  File "inference.py", line 98, in generator
    generation_output = model.generate(
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/transformers/src/transformers/generation/utils.py", line 1704, in generate
    return self.sample(
  File "/transformers/src/transformers/generation/utils.py", line 2786, in sample
    outputs = self(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/mistral/modeling_mistral.py", line 1048, in forward
    outputs = self.model(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/transformers/src/transformers/models/mistral/modeling_mistral.py", line 891, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File "/transformers/src/transformers/models/mistral/modeling_mistral.py", line 799, in _prepare_decoder_attention_mask
    combined_attention_mask = _make_sliding_window_causal_mask(
  File "/transformers/src/transformers/models/mistral/modeling_mistral.py", line 88, in _make_sliding_window_causal_mask
    mask = torch.triu(mask, diagonal=-sliding_window)
TypeError: bad operand type for unary -: 'NoneType'
