Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:32<07:03, 32.55s/it]Loading checkpoint shards:  14%|█▍        | 2/14 [01:09<06:59, 34.95s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [01:43<06:20, 34.58s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [02:16<05:39, 33.99s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [02:51<05:10, 34.53s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [03:24<04:30, 33.79s/it]Loading checkpoint shards:  50%|█████     | 7/14 [03:59<04:00, 34.39s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [04:24<03:07, 31.30s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [04:40<02:13, 26.61s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [05:00<01:37, 24.49s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [05:31<01:19, 26.51s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [06:04<00:56, 28.35s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [06:40<00:30, 30.84s/it]Loading checkpoint shards: 100%|██████████| 14/14 [06:54<00:00, 25.58s/it]Loading checkpoint shards: 100%|██████████| 14/14 [06:54<00:00, 29.59s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2024-01-09 21:12:44.932575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-09 21:12:45.618197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Finished loading PEFT model
finish loading model..
Generating!
Based on the news, you should buy Nvidia stocks, not sell.</s>
